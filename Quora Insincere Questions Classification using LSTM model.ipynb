{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___Quora Insincere Questions Classification___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world\n",
    "\n",
    "Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n",
    "\n",
    "In this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.\n",
    "\n",
    "# Link: https://www.kaggle.com/c/quora-insincere-questions-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chandan.S\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "#from tqdm.tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation, Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'embeddings', 'LSTM.ipynb', 'LSTM_Code.rar', 'model.h5', 'model.json', 'sample_submission.csv', 'submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape:  (1306122, 3)\n",
      "Train Data Shape:  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df  = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(\"Train Data Shape: \", train_df.shape)\n",
    "print(\"Train Data Shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Glove\n",
    "\n",
    "GlobalVectors (GloVe) is a model that learns vectors or words from their co-occurrence information. GloVe is a count-based model. This model that learns vectors or words from their co-occurrence information, i.e. how frequently they appear together in large text corpora, is GlobalVectors (GloVe).\n",
    "\n",
    "Count-based models learn vectors by doing dimensionality reduction on a co-occurrence counts matrix. First they construct a large matrix of co-occurrence information, which contains the information on how frequently each “word” (stored in rows), is seen in some “context” (the columns). The number of “contexts” needs be large, since it is essentially combinatorial in size. Afterwards they factorize this matrix to yield a lower-dimensional matrix of words and features, where each row yields a vector representation for each word. It is achieved by minimizing a “reconstruction loss” which looks for lower-dimensional representations that can explain the variance in the high-dimensional data.\n",
    "\n",
    "In the case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them. Compared to word2vec, GloVe allows for parallel implementation, which means that it’s easier to train over more data. It is believed (GloVe) to combine the benefits of the word2vec skip-gram model in the word analogy tasks, with those of matrix factorization methods exploiting global statistical information.\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://www.kdnuggets.com/2018/08/word-vectors-nlp-glove.html\n",
    "\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Glove : Dictionary of word and its coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"embeddings/glove.840B.300d/glove.840B.300d.txt\"\n",
    "embeddings_index = {} # Dictionary of word and its coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "f = open(EMBEDDING_FILE, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split(\" \")\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert values to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert values to embeddings\n",
    "def text_to_array(text):\n",
    "    empyt_emb = np.zeros(300)\n",
    "    text = text[:-1].split()[:30]\n",
    "    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n",
    "    embeds+= [empyt_emb] * (30 - len(embeds))\n",
    "    return np.array(embeds)\n",
    "\n",
    "val_vects = np.array([text_to_array(X_text) for X_text in val_df[\"question_text\"][:3000]])\n",
    "val_y = np.array(val_df[\"target\"][:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "def batch_gen(train_df):\n",
    "    n_batches = math.ceil(len(train_df) / batch_size)\n",
    "    while True: \n",
    "        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n",
    "        for i in range(n_batches):\n",
    "            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "            text_arr = np.array([text_to_array(text) for text in texts])\n",
    "            yield text_arr, np.array(train_df[\"target\"][i*batch_size:(i+1)*batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Architecture:\n",
    "\n",
    "__Sequential()__ : Initialize RNN.\n",
    "\n",
    "1) __Add 4 layers, with 100 units in each layer__\n",
    "\n",
    "2) __units__ : no of memory units you want to have in LSTM or number of LSTM cells\n",
    "\n",
    "3) __return_sequences__ will be set to \"True\" because we are building stacked RNN with multiple layers. If you want to add new LSTM layer after current layer then __return_sequences = True__ and if it is last layer then __return_sequences__ will be set to False\n",
    "\n",
    "4) __input_shape__ : Shape of x_train, but here we need not to give 3D shape, only shape corresponding to timestamps(2nd) and indicators(3rd) are needed. Shape corresponding to observation(1st) will automatically taken into account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# First Layer\n",
    "model.add(LSTM(units=100, return_sequences=True, input_shape=(30, 300)))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "#2nd Layer\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "#3rd Layer\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "#4th Layer\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#Output Layes\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile RNN\n",
    "model.compile(optimizer= 'adam',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 644s 644ms/step - loss: 0.1390 - acc: 0.9484 - val_loss: 0.1179 - val_acc: 0.9570\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 442s 442ms/step - loss: 0.1226 - acc: 0.9523 - val_loss: 0.1121 - val_acc: 0.9593\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 413s 413ms/step - loss: 0.1174 - acc: 0.9537 - val_loss: 0.1081 - val_acc: 0.9580\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 405s 405ms/step - loss: 0.1144 - acc: 0.9546 - val_loss: 0.1071 - val_acc: 0.9610\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 0.1112 - acc: 0.9569 - val_loss: 0.1083 - val_acc: 0.9613\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 0.1104 - acc: 0.9568 - val_loss: 0.1150 - val_acc: 0.9587\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 0.1106 - acc: 0.9568 - val_loss: 0.1051 - val_acc: 0.9627\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.1089 - acc: 0.9564 - val_loss: 0.1022 - val_acc: 0.9640\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 397s 397ms/step - loss: 0.1061 - acc: 0.9586 - val_loss: 0.1029 - val_acc: 0.9630\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 404s 404ms/step - loss: 0.1050 - acc: 0.9589 - val_loss: 0.1022 - val_acc: 0.9630\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 0.1035 - acc: 0.9590 - val_loss: 0.1010 - val_acc: 0.9620\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 0.1056 - acc: 0.9588 - val_loss: 0.1011 - val_acc: 0.9627\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.1019 - acc: 0.9593 - val_loss: 0.1010 - val_acc: 0.9620\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.1020 - acc: 0.9596 - val_loss: 0.1015 - val_acc: 0.9623\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 406s 406ms/step - loss: 0.1020 - acc: 0.9592 - val_loss: 0.1018 - val_acc: 0.9607\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 402s 402ms/step - loss: 0.1018 - acc: 0.9596 - val_loss: 0.1007 - val_acc: 0.9633\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 403s 403ms/step - loss: 0.1015 - acc: 0.9596 - val_loss: 0.1036 - val_acc: 0.9637\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 408s 408ms/step - loss: 0.1023 - acc: 0.9593 - val_loss: 0.1006 - val_acc: 0.9617\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 414s 414ms/step - loss: 0.0973 - acc: 0.9611 - val_loss: 0.1000 - val_acc: 0.9637\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 417s 417ms/step - loss: 0.0940 - acc: 0.9626 - val_loss: 0.1005 - val_acc: 0.9647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa389f13940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mg = batch_gen(train_df)\n",
    "model.fit_generator(mg, epochs=20,steps_per_epoch=1000,validation_data=(val_vects, val_y),verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the ability to describe any model using JSON format with a __to_json()__ function. This can be saved to file and later loaded via the __model_from_json()__ function that will create a new model from the JSON specification.\n",
    "\n",
    "The weights are saved directly from the model using the __save_weights()__ function and later loaded using the symmetrical __load_weights()__ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then converted to JSON format and written to __model.json__ in the local directory. The network weights are written to __model.h5__ in the local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize model to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# serialize weights to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved to disk\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "print(\"Model Saved to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load json and create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous step model and weights are save. If we want to use previously saved model then we can do so by loading and compiling it. In this training will not happen again. To do so download __model.json__ and __model.h5__ is same folder and uncomment and run below code. \n",
    "\n",
    "\n",
    "The weights are saved directly from the model using the __save_weights()__ function and later loaded using the symmetrical __load_weights()__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json_file = open('model.json', 'r')\n",
    "#loaded_model_json = json_file.read()\n",
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "# Load saved weights\n",
    "#loaded_model.load_weights(\"model.h5\")\n",
    "#print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model after loading from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The model and weight data is loaded from the saved files and a new model is created. It is important to compile the loaded model before it is used. This is so that predictions made using the model can use the appropriate efficient computation from the Keras backend.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer= 'adam',loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "def batch_gen(test_df):\n",
    "    n_batches = math.ceil(len(test_df) / batch_size)\n",
    "    for i in range(n_batches):\n",
    "        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n",
    "        text_arr = np.array([text_to_array(text) for text in texts])\n",
    "        yield text_arr\n",
    "\n",
    "all_preds = []\n",
    "for x in batch_gen(test_df):\n",
    "    all_preds.extend(model.predict(x).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = (np.array(all_preds) > 0.5).astype(np.int)\n",
    "\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_test})\n",
    "submit_df.to_csv(\"Output.csv\", index=False)  # Output.csv will have prediction of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ___END___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
